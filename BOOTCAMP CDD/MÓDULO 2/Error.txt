{
	"name": "Py4JJavaError",
	"message": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 4.0 failed 1 times, most recent failure: Lost task 1.0 in stage 4.0 (TID 5) (DESKTOP-C2A46PP executor driver): org.apache.spark.SparkException: Python worker failed to connect back.\r
\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:203)\r
\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\r
\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\r
\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:174)\r
\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:67)\r
\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r
\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r
\tat org.apache.spark.api.python.PairwiseRDD.compute(PythonRDD.scala:130)\r
\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r
\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r
\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r
\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\r
\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\r
\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\r
\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r
\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r
\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r
\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r
\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r
\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r
\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r
\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r
\tat java.lang.Thread.run(Unknown Source)\r
Caused by: java.net.SocketTimeoutException: Accept timed out\r
\tat java.net.DualStackPlainSocketImpl.waitForNewConnection(Native Method)\r
\tat java.net.DualStackPlainSocketImpl.socketAccept(Unknown Source)\r
\tat java.net.AbstractPlainSocketImpl.accept(Unknown Source)\r
\tat java.net.PlainSocketImpl.accept(Unknown Source)\r
\tat java.net.ServerSocket.implAccept(Unknown Source)\r
\tat java.net.ServerSocket.accept(Unknown Source)\r
\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:190)\r
\t... 22 more\r

Driver stacktrace:\r
\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\r
\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\r
\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\r
\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r
\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r
\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r
\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\r
\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\r
\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\r
\tat scala.Option.foreach(Option.scala:407)\r
\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\r
\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\r
\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\r
\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\r
\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r
\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)\r
\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)\r
\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2419)\r
\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2438)\r
\tat org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:181)\r
\tat org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)\r
\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r
\tat sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\r
\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\r
\tat java.lang.reflect.Method.invoke(Unknown Source)\r
\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r
\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r
\tat py4j.Gateway.invoke(Gateway.java:282)\r
\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r
\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r
\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r
\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r
\tat java.lang.Thread.run(Unknown Source)\r
Caused by: org.apache.spark.SparkException: Python worker failed to connect back.\r
\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:203)\r
\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\r
\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\r
\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:174)\r
\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:67)\r
\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r
\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r
\tat org.apache.spark.api.python.PairwiseRDD.compute(PythonRDD.scala:130)\r
\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r
\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r
\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r
\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\r
\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\r
\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\r
\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r
\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r
\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r
\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r
\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r
\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r
\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r
\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r
\t... 1 more\r
Caused by: java.net.SocketTimeoutException: Accept timed out\r
\tat java.net.DualStackPlainSocketImpl.waitForNewConnection(Native Method)\r
\tat java.net.DualStackPlainSocketImpl.socketAccept(Unknown Source)\r
\tat java.net.AbstractPlainSocketImpl.accept(Unknown Source)\r
\tat java.net.PlainSocketImpl.accept(Unknown Source)\r
\tat java.net.ServerSocket.implAccept(Unknown Source)\r
\tat java.net.ServerSocket.accept(Unknown Source)\r
\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:190)\r
\t... 22 more\r
",
	"stack": "---------------------------------------------------------------------------
Py4JJavaError                             Traceback (most recent call last)
Cell In[20], line 18
     12 counts = linesRdd.flatMap(lambda x: x.split(' '))\\
     13                     .map(lambda x: (x, 1))\\
     14                     .reduceByKey(add)
     16 path = \"C:\\\\Users\\\\Matheus Poletto\\\\Desktop\\\\Cientista de Dados\\\\POS XP\\\\BOOTCAMP CDD\\\\MÃ“DULO 2\\\\\"
---> 18 output = counts.take(10)
     19 for (number, count) in output:
     20     print(\"%s: %i\" % (number, count))

File c:\\Users\\Matheus Poletto\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pyspark\\rdd.py:2855, in RDD.take(self, num)
   2852         taken += 1
   2854 p = range(partsScanned, min(partsScanned + numPartsToTry, totalParts))
-> 2855 res = self.context.runJob(self, takeUpToNumLeft, p)
   2857 items += res
   2858 partsScanned += numPartsToTry

File c:\\Users\\Matheus Poletto\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pyspark\\context.py:2510, in SparkContext.runJob(self, rdd, partitionFunc, partitions, allowLocal)
   2508 mappedRDD = rdd.mapPartitions(partitionFunc)
   2509 assert self._jvm is not None
-> 2510 sock_info = self._jvm.PythonRDD.runJob(self._jsc.sc(), mappedRDD._jrdd, partitions)
   2511 return list(_load_from_socket(sock_info, mappedRDD._jrdd_deserializer))

File c:\\Users\\Matheus Poletto\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\py4j\\java_gateway.py:1322, in JavaMember.__call__(self, *args)
   1316 command = proto.CALL_COMMAND_NAME +\\
   1317     self.command_header +\\
   1318     args_command +\\
   1319     proto.END_COMMAND_PART
   1321 answer = self.gateway_client.send_command(command)
-> 1322 return_value = get_return_value(
   1323     answer, self.gateway_client, self.target_id, self.name)
   1325 for temp_arg in temp_args:
   1326     if hasattr(temp_arg, \"_detach\"):

File c:\\Users\\Matheus Poletto\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py:179, in capture_sql_exception.<locals>.deco(*a, **kw)
    177 def deco(*a: Any, **kw: Any) -> Any:
    178     try:
--> 179         return f(*a, **kw)
    180     except Py4JJavaError as e:
    181         converted = convert_exception(e.java_exception)

File c:\\Users\\Matheus Poletto\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\py4j\\protocol.py:326, in get_return_value(answer, gateway_client, target_id, name)
    324 value = OUTPUT_CONVERTER[type](answer[2:], gateway_client)
    325 if answer[1] == REFERENCE_TYPE:
--> 326     raise Py4JJavaError(
    327         \"An error occurred while calling {0}{1}{2}.\
\".
    328         format(target_id, \".\", name), value)
    329 else:
    330     raise Py4JError(
    331         \"An error occurred while calling {0}{1}{2}. Trace:\
{3}\
\".
    332         format(target_id, \".\", name, value))

Py4JJavaError: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 4.0 failed 1 times, most recent failure: Lost task 1.0 in stage 4.0 (TID 5) (DESKTOP-C2A46PP executor driver): org.apache.spark.SparkException: Python worker failed to connect back.\r
\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:203)\r
\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\r
\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\r
\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:174)\r
\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:67)\r
\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r
\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r
\tat org.apache.spark.api.python.PairwiseRDD.compute(PythonRDD.scala:130)\r
\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r
\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r
\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r
\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\r
\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\r
\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\r
\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r
\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r
\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r
\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r
\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r
\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r
\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r
\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r
\tat java.lang.Thread.run(Unknown Source)\r
Caused by: java.net.SocketTimeoutException: Accept timed out\r
\tat java.net.DualStackPlainSocketImpl.waitForNewConnection(Native Method)\r
\tat java.net.DualStackPlainSocketImpl.socketAccept(Unknown Source)\r
\tat java.net.AbstractPlainSocketImpl.accept(Unknown Source)\r
\tat java.net.PlainSocketImpl.accept(Unknown Source)\r
\tat java.net.ServerSocket.implAccept(Unknown Source)\r
\tat java.net.ServerSocket.accept(Unknown Source)\r
\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:190)\r
\t... 22 more\r

Driver stacktrace:\r
\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\r
\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\r
\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\r
\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r
\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r
\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r
\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\r
\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\r
\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\r
\tat scala.Option.foreach(Option.scala:407)\r
\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\r
\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\r
\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\r
\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\r
\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r
\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)\r
\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)\r
\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2419)\r
\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2438)\r
\tat org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:181)\r
\tat org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)\r
\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r
\tat sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\r
\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\r
\tat java.lang.reflect.Method.invoke(Unknown Source)\r
\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r
\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r
\tat py4j.Gateway.invoke(Gateway.java:282)\r
\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r
\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r
\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r
\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r
\tat java.lang.Thread.run(Unknown Source)\r
Caused by: org.apache.spark.SparkException: Python worker failed to connect back.\r
\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:203)\r
\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\r
\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\r
\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:174)\r
\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:67)\r
\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r
\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r
\tat org.apache.spark.api.python.PairwiseRDD.compute(PythonRDD.scala:130)\r
\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r
\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r
\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r
\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\r
\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\r
\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\r
\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r
\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r
\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r
\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r
\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r
\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r
\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r
\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r
\t... 1 more\r
Caused by: java.net.SocketTimeoutException: Accept timed out\r
\tat java.net.DualStackPlainSocketImpl.waitForNewConnection(Native Method)\r
\tat java.net.DualStackPlainSocketImpl.socketAccept(Unknown Source)\r
\tat java.net.AbstractPlainSocketImpl.accept(Unknown Source)\r
\tat java.net.PlainSocketImpl.accept(Unknown Source)\r
\tat java.net.ServerSocket.implAccept(Unknown Source)\r
\tat java.net.ServerSocket.accept(Unknown Source)\r
\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:190)\r
\t... 22 more\r
"
}